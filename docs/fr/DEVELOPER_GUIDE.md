# Guide DÃ©veloppeur Lyra Clean

Guide complet pour comprendre, modifier et contribuer au framework Lyra Clean.

## Table des matiÃ¨res

1. [Architecture](#architecture)
2. [Structure du code](#structure-du-code)
3. [Composants principaux](#composants-principaux)
4. [Flux de donnÃ©es](#flux-de-donnÃ©es)
5. [Moteur physique BÃ©zier](#moteur-physique-bÃ©zier)
6. [SystÃ¨me de conscience](#systÃ¨me-de-conscience)
7. [Base de donnÃ©es](#base-de-donnÃ©es)
8. [Tests et benchmarks](#tests-et-benchmarks)
9. [Contribution](#contribution)
10. [Roadmap](#roadmap)

---

## Architecture

### Vue d'ensemble

Lyra Clean suit une architecture en couches (layered architecture) avec sÃ©paration claire des responsabilitÃ©s :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Application                      â”‚
â”‚                   (app/main.py, app/api/)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Services Layer                           â”‚
â”‚         (Context Injection, Consciousness, Memory)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Database Layer                            â”‚
â”‚            (SQLite Engine, Schema Management)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Core Physics Engine                        â”‚
â”‚         (BÃ©zier Trajectories, Parameter Mapping)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Principes de conception

1. **Deterministic Physics**
   - ParamÃ¨tres LLM contrÃ´lÃ©s par trajectoires mathÃ©matiques
   - Comportement prÃ©visible et reproductible
   - Pas de feedback loops rÃ©actifs

2. **Separation of Concerns**
   - API layer : Validation, routing
   - Services : Business logic, context injection
   - Database : Persistence, queries
   - Core : Pure math, no side effects

3. **Async-First**
   - Tout est async/await (aiosqlite, httpx)
   - Non-blocking I/O
   - ScalabilitÃ© horizontale

4. **Dependency Injection**
   - FastAPI `Depends()` pour DI
   - Singleton pattern pour DB et LLM client
   - Testable et modulaire

5. **Type Safety**
   - Pydantic models pour validation
   - Type hints partout
   - Mypy-friendly (mostly)

---

## Structure du code

```
lyra_clean_bis/
â”‚
â”œâ”€â”€ app/                          # Application FastAPI
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                   # Entry point (369 LOC)
â”‚   â”œâ”€â”€ models.py                 # Pydantic models (307 LOC)
â”‚   â”œâ”€â”€ llm_client.py             # Ollama async client (308 LOC)
â”‚   â”œâ”€â”€ embeddings.py             # Embedding wrapper (91 LOC)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat.py               # Chat endpoint (350 LOC)
â”‚   â”‚   â”œâ”€â”€ sessions.py           # Session management (332 LOC)
â”‚   â”‚   â”œâ”€â”€ graph.py              # [NOUVEAU] API mutations graphe (Lyra-ACE)
â”‚   â”‚   â””â”€â”€ multimodel.py         # [NOUVEAU] API multi-modÃ¨les
â”‚   â””â”€â”€ static/
â”‚       â””â”€â”€ index.html            # Web UI
â”‚
â”œâ”€â”€ services/                     # Business logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ injector.py               # Context injection (442 LOC)
â”‚   â”œâ”€â”€ entity_resolver.py        # [NOUVEAU] DÃ©duplication sÃ©mantique
â”‚   â”œâ”€â”€ relation_normalizer.py    # [NOUVEAU] Canonicalisation relations
â”‚   â”œâ”€â”€ kappa_worker.py           # [NOUVEAU] Calcul courbure asynchrone
â”‚   â”œâ”€â”€ session_storage.py        # [NOUVEAU] Export/import sessions
â”‚   â””â”€â”€ consciousness/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ metrics.py            # Phase 1: Passive monitoring
â”‚       â”œâ”€â”€ adaptation.py         # Phase 2: Active adaptation
â”‚       â””â”€â”€ memory.py             # Phase 3: Semantic memory
â”‚
â”œâ”€â”€ database/                     # Data layer
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ engine.py                 # ISpaceDB (571 LOC)
â”‚   â”œâ”€â”€ schema.sql                # Database schema
â”‚   â”œâ”€â”€ graph_delta.py            # [NOUVEAU] Suivi mutations graphe
â”‚   â”œâ”€â”€ models.py                 # [NOUVEAU] ModÃ¨les Pydantic
â”‚   â””â”€â”€ pool.py                   # [NOUVEAU] Pool connexions & cache
â”‚
â”œâ”€â”€ core/                         # Pure logic
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ security.py               # [NOUVEAU] Gestion secrets
â”‚   â””â”€â”€ physics/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ bezier.py             # BÃ©zier engine (471 LOC)
â”‚
â”œâ”€â”€ data/                         # Runtime data
â”‚   â”œâ”€â”€ ispace.db                 # SQLite database
â”‚   â”œâ”€â”€ embeddings_cache.json    # Embeddings cache
â”‚   â””â”€â”€ weaver.log                # Application logs
â”‚
â”œâ”€â”€ saves/                        # [NOUVEAU] Exports sessions
â”‚   â””â”€â”€ {nom_modele}/             # OrganisÃ© par modÃ¨le LLM
â”‚       â””â”€â”€ {timestamp}_{id}.json
â”‚
â”œâ”€â”€ scripts/                      # Utilities
â”‚   â”œâ”€â”€ build_global_map.py      # Import knowledge graph
â”‚   â”œâ”€â”€ test_api.py               # API tests
â”‚   â””â”€â”€ test_brain.py             # Physics tests
â”‚
â”œâ”€â”€ tests/                        # Unit tests
â”‚   â””â”€â”€ test_ab_metrics.py
â”‚
â”œâ”€â”€ config.yaml                   # Configuration
â”œâ”€â”€ requirements.txt              # Dependencies
â”œâ”€â”€ docker-compose.yml            # Docker setup
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

### Statistiques du codebase

| Composant | Fichiers | LOC | ComplexitÃ© |
|-----------|----------|-----|------------|
| **App** | 8 | ~1,800 | Moyenne |
| **Services** | 8 | ~1,200 | Ã‰levÃ©e |
| **Database** | 5 | ~1,100 | Moyenne |
| **Core** | 2 | ~600 | Ã‰levÃ©e |
| **Total** | 23 | ~4,700 | - |

---

## Composants principaux

### 1. Application Layer (app/)

#### main.py

Point d'entrÃ©e FastAPI avec lifecycle management.

**ResponsabilitÃ©s :**
- Initialisation database et LLM client
- Configuration CORS
- Mounting static files
- Health checks

**Hooks de lifecycle :**
```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await get_database()
    await get_ollama_client(...)
    yield
    # Shutdown
    await close_ollama_client()
```

**Endpoints montÃ©s :**
- `/` : Web UI
- `/api` : Root API endpoint
- `/health` : Health checks
- `/stats` : System stats
- `/chat/*` : Chat router
- `/sessions/*` : Sessions router
- `/profiles/*` : Profiles router

#### models.py

Pydantic models pour validation de requÃªtes/rÃ©ponses.

**ModÃ¨les principaux :**
```python
# Requests
ChatRequest(message, session_id?, consciousness_level?, ...)
SessionCreateRequest(profile_name?, max_messages?, ...)

# Responses
ChatResponse(response, session_id, physics_state, ...)
SessionResponse(session_id, created_at, ...)
ProfileResponse(name, description, curves)
HealthResponse(status, database, ollama)

# Domain models
PhysicsState(t, tau_c, rho, delta_r, kappa?)
ConsciousnessMetrics(coherence, tension, fit, ...)
```

**Validators :**
```python
@field_validator('consciousness_level')
def validate_consciousness_level(cls, v):
    if not 0 <= v <= 3:
        raise ValueError("Must be 0-3")
    return v
```

#### llm_client.py

Client async pour Ollama API.

**Features :**
- Connection pooling (httpx)
- Retry logic avec backoff exponentiel
- Timeout handling
- Physics parameter mapping

**Utilisation :**
```python
client = await get_ollama_client()
response = await client.chat(
    messages=[{"role": "user", "content": "..."}],
    physics_state=state
)
```

**Mapping physique â†’ Ollama :**
```python
temperature = map_tau_to_temperature(tau_c)  # [0.1, 1.5]
repeat_penalty = 1.0 + map_rho_to_penalties(rho)["frequency_penalty"]
```

#### embeddings.py

Wrapper pour gÃ©nÃ©ration d'embeddings (mxbai-embed-large, 1024D).

```python
# Single text
emb = await get_embeddings("Hello world")  # shape: (1024,)

# Batch (sequential pour l'instant)
embs = await get_embeddings_batch(["text1", "text2", ...])
```

### 2. Services Layer (services/)

#### injector.py

Injection de contexte sÃ©mantique depuis le graphe de connaissances.

**Workflow :**
```
User message
    â†“
Extract keywords (TF-IDF, stop words)
    â†“
Query semantic neighbors (SQLite, PPMI weights)
    â†“
Schedule context based on Î´_r
    â†“
Inject into system prompt
```

**Classes principales :**
```python
class ContextInjector:
    async def inject_context(self, message, physics_state, db) -> GraphContext
        # Returns GraphContext(neighbors, keywords, total_weight)

class ConversationMemory:
    async def format_history(self, session_id, max_messages, max_tokens) -> List[Dict]
        # Returns conversation history with token budget
```

**Keyword extraction :**
```python
def extract_keywords(text: str, max_keywords: int) -> List[str]:
    # TF-IDF-like scoring
    # Stop words filtering (English + French)
    # Returns top N keywords
```

#### consciousness/metrics.py

Phase 1 : Calcul de mÃ©triques Ã©pistemologiques (passive, no side effects).

**MÃ©triques :**
```python
class ConsciousnessMonitor:
    async def compute_metrics(
        self,
        context: GraphContext,
        response: str,
        physics_state: PhysicsState,
        message_index: int
    ) -> ConsciousnessMetrics
```

**Formules :**
- **Coherence** : `min(1.0, total_weight / 10.0)`
- **Tension** : `tau_c * log(1 + len(response) / 500)`
- **Fit** : `1.0 - abs(expected_len - actual_len) / max(expected_len, actual_len)`
- **Pressure** : `(tau_c + delta_r) / 2.0`
- **Stability** : Composite score basÃ© sur coherence, tension, fit

#### consciousness/adaptation.py

Phase 2 : Adaptation active (suggÃ¨re ajustements).

**RÃ¨gles d'adaptation :**
```python
def suggest_adaptation(self, metrics, state, message_index):
    if metrics.tension > 0.75:
        # Reduce tau_c by 5%
        return Suggestion(reason="High tension", adjustments={"tau_c": -0.05})

    if metrics.coherence < 0.3:
        # Adjust rho towards focus
        return Suggestion(...)

    if metrics.fit > 0.8 and metrics.stability_score > 0.7:
        # Encourage exploration
        return Suggestion(...)

    # ... etc
```

**CaractÃ©ristiques :**
- Ajustements graduels (5-7.5% par tour)
- RÃ¨gles non-conflictuelles
- Convergence garantie (long sessions)

#### consciousness/memory.py

Phase 3 : MÃ©moire sÃ©mantique avec rappel par similaritÃ©.

**Architecture :**
```python
@dataclass
class MemoryEntry:
    content: str
    embedding: np.ndarray  # 1024D
    timestamp: datetime
    message_index: int

class SemanticMemory:
    _memories: Dict[str, List[MemoryEntry]]  # session_id -> entries

    async def record(self, session_id, content, message_index):
        # Generate embedding, store in dict

    async def recall(self, session_id, query_text, threshold=0.7, max_results=3):
        # Cosine similarity search
        # Temporal decay: max(0.5, 1.0 - turns_ago * 0.01)
        # Return top matches
```

**Injection dans contexte :**
```python
# AjoutÃ© au system prompt
[MEMORY ECHO] (similarity=0.89, 12 turns ago):
{recalled_content}
```

#### Services Lyra-ACE (Nouveau)

**entity_resolver.py** - DÃ©duplication sÃ©mantique d'entitÃ©s

RÃ©sout les concepts vers leur forme canonique via similaritÃ© d'embeddings.

```python
class EntityResolver:
    async def resolve(self, concept: str, auto_create: bool = True) -> ResolutionResult

# StratÃ©gie de rÃ©solution :
# 1. VÃ©rifier les aliases existants (match exact)
# 2. VÃ©rifier le concept directement (match exact)
# 3. Chercher par similaritÃ© d'embedding
# 4. CrÃ©er si nouveau (auto_create=True)

# Seuils :
SIMILARITY_THRESHOLD = 0.92  # Fusion automatique
REVIEW_THRESHOLD = 0.85      # Candidat Ã  la revue
```

**relation_normalizer.py** - Canonicalisation des relations

Mappe les relations brutes vers 20 formes canoniques avec gestion des inverses et symÃ©trie.

```python
class RelationNormalizer:
    async def normalize(self, relation: str) -> str
    async def get_inverse(self, relation: str) -> Optional[str]
    async def is_symmetric(self, relation: str) -> bool
    async def get_category(self, relation: str) -> str

# CatÃ©gories : causal, hierarchical, associative, property,
#             temporal, epistemic, transformational, comparative

# Exemples de mappings :
# "provoque" -> "cause"
# "est un" -> "is_a"
# "cause" <-> "caused_by" (paire inverse)
```

**kappa_worker.py** - Calcul de courbure asynchrone

Worker en arriÃ¨re-plan pour calcul batch de courbure Ollivier.

```python
class KappaWorker:
    def __init__(self, db: ISpaceDB, alpha: float = 0.5):
        self.calculator = KappaCalculator(alpha=alpha)

    async def process_batch(self, limit: int = 100) -> int:
        # Traite les arÃªtes en attente, retourne le nombre traitÃ©

    async def run_continuous(self, interval: float = 5.0):
        # ExÃ©cute comme worker en arriÃ¨re-plan

# StratÃ©gie :
# - InsÃ©rer arÃªtes avec kappa Jaccard (rapide, O(1))
# - Calculer kappa Ollivier en arriÃ¨re-plan (diffÃ©rÃ©)
# - Mettre Ã  jour avec kappa hybride quand prÃªt
```

**session_storage.py** - Export/Import de sessions

Persiste les sessions vers fichiers JSON organisÃ©s par modÃ¨le.

```python
class SessionStorage:
    def __init__(self, base_dir: str = "saves"):
        # Organisation : saves/{nom_modele}/{timestamp}_{session_id}.json

    async def export_session(self, db, session_id: str, model: str) -> Dict:
        # Exporte : messages, trajectoires, ajustements conscience

    async def import_session(self, db, filepath: str, new_session_id: Optional[str]) -> Dict:
        # Restaure session avec ID nouveau ou spÃ©cifiÃ©

    def list_saves(self, model: Optional[str] = None) -> List[Dict]:
        # Liste les sauvegardes, optionnellement filtrÃ©es par modÃ¨le
```

### 3. Database Layer (database/)

#### engine.py

Unified async SQLite engine.

**Classe principale :**
```python
class ISpaceDB:
    def __init__(self, db_path: str):
        self._db_path = db_path
        self._pool = None  # aiosqlite connection pool

    async def initialize(self):
        # Create tables if not exists
        # Enable WAL mode
        # Create indexes
        # Optimize PRAGMA settings

    # Concept queries
    async def get_concept(self, concept: str) -> Dict
    async def get_neighbors(self, concept: str, limit: int) -> List[Dict]
    async def search_concepts(self, keyword: str) -> List[str]

    # Session management
    async def create_session(self, profile_name: str) -> str
    async def get_session(self, session_id: str) -> Dict
    async def append_event(self, session_id, role, content, ...)

    # Profile management
    async def get_profile(self, name: str) -> Dict
    async def list_profiles(self) -> List[Dict]

    # Utilities
    async def get_stats(self) -> Dict
    async def vacuum(self)  # VACUUM + ANALYZE
```

**Optimisations :**
```sql
-- WAL mode for concurrent reads
PRAGMA journal_mode=WAL;

-- Large cache
PRAGMA cache_size=-64000;  -- 64MB

-- Memory-mapped I/O
PRAGMA mmap_size=268435456;  -- 256MB
```

**Indexes :**
```sql
-- O(log N) lookups
CREATE INDEX idx_relations_source ON semantic_relations(source);
CREATE INDEX idx_events_session ON events(session_id, timestamp);
CREATE INDEX idx_sessions_created ON sessions(created_at);
```

#### schema.sql

SchÃ©ma de base de donnÃ©es (13KB).

**Tables principales :**

```sql
-- Knowledge graph
concepts (
    concept TEXT PRIMARY KEY,
    embedding BLOB  -- 1024D float32 array (optionnel)
)

semantic_relations (
    source TEXT,
    target TEXT,
    weight REAL,  -- PPMI score
    PRIMARY KEY (source, target)
)

-- Sessions
sessions (
    session_id TEXT PRIMARY KEY,
    created_at TEXT,
    profile_name TEXT,
    max_messages INTEGER,
    time_mapping TEXT
)

events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    timestamp TEXT,
    role TEXT,  -- user, assistant, system
    content TEXT,
    message_index INTEGER,
    physics_state TEXT  -- JSON
)

-- BÃ©zier profiles
bezier_profiles (
    name TEXT PRIMARY KEY,
    description TEXT,
    tau_c_json TEXT,  -- 4 control points
    rho_json TEXT,
    delta_r_json TEXT,
    kappa_json TEXT  -- optionnel
)

-- Trajectory logging (for analysis)
trajectory_log (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    message_index INTEGER,
    t REAL,
    tau_c REAL,
    rho REAL,
    delta_r REAL,
    kappa REAL
)
```

#### graph_delta.py (Nouveau)

Gestion des mutations atomiques du graphe avec piste d'audit.

```python
class DeltaOperation(Enum):
    ADD_NODE = "add_node"
    ADD_EDGE = "add_edge"
    UPDATE_NODE = "update_node"
    UPDATE_EDGE = "update_edge"
    DELETE_NODE = "delete_node"
    DELETE_EDGE = "delete_edge"

@dataclass
class GraphDelta:
    operation: DeltaOperation
    source: str
    target: Optional[str] = None
    weight: Optional[float] = None
    confidence: float = 1.0
    model_source: str = "system"
    reason: Optional[str] = None

@dataclass
class DeltaBatch:
    deltas: List[GraphDelta]
    max_mutation_ratio: float = 0.05  # Limite 5% par batch

class KappaCalculator:
    """Calculateur courbure hybride (Ollivier + Jaccard)"""

    def ollivier_approx(self, degree_u, degree_v, weight) -> float:
        # kappa = 1/deg(u) + 1/deg(v) - 2/w

    def jaccard_kappa(self, neighbors_u, neighbors_v) -> float:
        # kappa = |N(u) âˆ© N(v)| / |N(u) âˆª N(v)|

    def compute_hybrid(self, ...) -> Dict[str, float]:
        # Retourne : kappa_ollivier, kappa_jaccard, kappa_hybrid, alpha
```

#### pool.py (Nouveau)

Pool de connexions et utilitaires de performance.

```python
class SQLiteConnectionPool:
    """Pool de connexions async avec gestion overflow"""

    def __init__(self, db_path: str, pool_size: int = 10, max_overflow: int = 5):
        self._pragmas = [
            "PRAGMA journal_mode=WAL",
            "PRAGMA synchronous=NORMAL",
            "PRAGMA cache_size=-64000",
            "PRAGMA mmap_size=268435456",
        ]

    async def acquire(self):
        # Context manager pour acquisition connexion

class ConceptCache:
    """Cache LRU avec TTL pour concepts (1000 entrÃ©es, 1h TTL)"""

class ConcurrencyLimiter:
    """ContrÃ´le concurrence via Semaphore"""

class SQLValidator:
    """PrÃ©vention injection SQL"""

    @classmethod
    def validate_identifier(cls, value: str) -> bool
    @classmethod
    def sanitize_string(cls, value: str) -> str
    @classmethod
    def validate_concept_id(cls, concept_id: str) -> str
```

### 4. Core Layer (core/)

#### security.py (Nouveau)

Gestion sÃ©curisÃ©e des secrets et clÃ©s API.

```python
from core.security import get_api_key, validate_environment, init_security

# Au dÃ©marrage
validate_environment()

# RÃ©cupÃ©rer clÃ©s API
ollama_url = get_api_key("OLLAMA_URL", default="http://localhost:11434")
mistral_key = get_api_key("MISTRAL_API_KEY")

# Variables d'environnement attendues :
# OLLAMA_URL       - URL serveur Ollama
# OLLAMA_MODEL     - ModÃ¨le par dÃ©faut
# MISTRAL_API_KEY  - API Mistral (optionnel)
# OPENAI_API_KEY   - API OpenAI (optionnel)
# LYRA_SECRET_KEY  - ClÃ© secrÃ¨te application
# LYRA_ENV         - Environnement (development/production)
# LYRA_DEBUG       - Mode debug

# Utilitaires :
mask_secret("sk-abc123def456")  # Retourne "************f456"
generate_session_token()         # Retourne token hex 64 caractÃ¨res
is_production()                  # VÃ©rifie si en production
```

### 5. Moteur Physique (core/physics/)

#### bezier.py

Moteur de trajectoires BÃ©zier (pure math, no side effects).

**Classes principales :**

```python
@dataclass(frozen=True)
class BezierPoint:
    t: float      # Parameter [0, 1]
    value: float  # Value [0, 1]

class CubicBezier:
    """Cubic BÃ©zier curve interpolator."""

    def __init__(self, points: List[BezierPoint]):
        # Must have exactly 4 control points
        # Validate monotonicity and endpoints

    def evaluate(self, t: float) -> float:
        """Evaluate curve at parameter t âˆˆ [0, 1]."""
        # De Casteljau's algorithm

    def derivative(self, t: float) -> float:
        """Rate of change at t."""

    @classmethod
    def from_json(cls, json_str: str) -> 'CubicBezier':
        """Deserialize from JSON array."""

@dataclass(frozen=True)
class PhysicsState:
    t: float        # Normalized time [0, 1]
    tau_c: float    # Tension/temperature
    rho: float      # Focus/polarity
    delta_r: float  # Scheduling
    kappa: Optional[float] = None  # Curvature/style

class BezierEngine:
    """Main trajectory engine."""

    def __init__(
        self,
        tau_c_curve: CubicBezier,
        rho_curve: CubicBezier,
        delta_r_curve: CubicBezier,
        kappa_curve: Optional[CubicBezier] = None
    ):
        self.curves = {...}

    def compute_state(self, t: float) -> PhysicsState:
        """Compute physics state at normalized time t."""
        return PhysicsState(
            t=t,
            tau_c=self.curves['tau_c'].evaluate(t),
            rho=self.curves['rho'].evaluate(t),
            delta_r=self.curves['delta_r'].evaluate(t),
            kappa=self.curves.get('kappa')?.evaluate(t)
        )

    def sample_trajectory(self, num_points: int) -> List[PhysicsState]:
        """Sample trajectory for visualization."""

class TimeMapper:
    """Map message count to normalized time t âˆˆ [0, 1]."""

    @staticmethod
    def linear(n: int, max_n: int) -> float:
        return n / max_n

    @staticmethod
    def logarithmic(n: int, max_n: int) -> float:
        # Slower early progress
        return math.log(1 + n) / math.log(1 + max_n)

    @staticmethod
    def sigmoid(n: int, max_n: int) -> float:
        # Smooth S-curve
        x = (n / max_n - 0.5) * 10
        return 1.0 / (1.0 + math.exp(-x))
```

**Parameter mappers :**
```python
def map_tau_to_temperature(tau_c: float) -> float:
    """Map tau_c âˆˆ [0, 1] to temperature âˆˆ [0.1, 1.5]."""
    return 0.1 + tau_c * 1.4

def map_rho_to_penalties(rho: float) -> Dict[str, float]:
    """Map rho to presence/frequency penalties."""
    # rho=0.5 â†’ neutral
    # rho<0.5 â†’ more repetition allowed
    # rho>0.5 â†’ penalize repetition
    return {
        "presence_penalty": (rho - 0.5) * 0.4,
        "frequency_penalty": (rho - 0.5) * 0.6
    }

def map_kappa_to_style_hints(kappa: float) -> str:
    """Generate style hints for prompt."""
    if kappa < 0.3:
        return "Be concise and direct."
    elif kappa > 0.7:
        return "Elaborate deeply with examples."
    else:
        return ""
```

---

## Flux de donnÃ©es

### Chat Request Flow

```
1. POST /chat/message
   â†“
2. Pydantic validation (ChatRequest)
   â†“
3. Session lookup or create
   â†“
4. Compute physics state (BÃ©zier curves + time mapping)
   â†“
5. Retrieve conversation history (sliding window)
   â†“
6. Extract keywords from user message
   â†“
7. Query semantic neighbors (database)
   â†“
8. [If level â‰¥ 3] Recall similar past messages
   â†“
9. Assemble enriched prompt:
   - System prompt + semantic context + memory echoes
   - Conversation history
   - User message
   â†“
10. LLM API call (Ollama) with physics parameters
   â†“
11. Log event to database (user + assistant messages)
   â†“
12. [If level â‰¥ 1] Compute consciousness metrics
   â†“
13. [If level = 2] Generate adaptation suggestion
   â†“
14. Log trajectory point
   â†“
15. Return ChatResponse
```

### Database Query Flow

**Semantic neighbor query :**
```sql
-- O(k log N) where k = number of keywords
WITH neighbors AS (
    SELECT target AS concept, weight
    FROM semantic_relations
    WHERE source IN (keyword1, keyword2, ...)
      AND weight > min_weight
    ORDER BY weight DESC
    LIMIT max_neighbors
)
SELECT * FROM neighbors
```

**Conversation history retrieval :**
```sql
-- O(log N) via idx_events_session
SELECT role, content, timestamp, message_index, physics_state
FROM events
WHERE session_id = ?
ORDER BY message_index DESC
LIMIT ?
```

### Memory Recall Flow

```
1. User message arrives
   â†“
2. Generate embedding (mxbai-embed-large)
   â†“
3. Compute cosine similarity with all past messages in session
   â†“
4. Apply temporal decay: max(0.5, 1.0 - turns_ago * 0.01)
   â†“
5. Filter by threshold (0.7)
   â†“
6. Return top 3 matches
   â†“
7. Inject as [MEMORY ECHO] in system prompt
```

---

## Moteur physique BÃ©zier

### Pourquoi des courbes de BÃ©zier ?

**Avantages :**
1. **ContrÃ´le intuitif** : 4 points dÃ©finissent toute la trajectoire
2. **Interpolation lisse** : DÃ©rivÃ©es continues (C1)
3. **Contraintes naturelles** : t âˆˆ [0, 1], valeurs bornÃ©es
4. **Visualisable** : Facile Ã  prÃ©visualiser et dÃ©bugger
5. **Efficace** : Ã‰valuation O(1) via de Casteljau

**Alternatives considÃ©rÃ©es :**
- PolynÃ´mes : Instables (Runge phenomenon)
- Splines : Trop complexe pour 4 points
- LinÃ©aire par morceaux : Pas assez lisse

### Anatomie d'une courbe

**4 points de contrÃ´le :**
```
P0 (t=0)    : Point de dÃ©part (valeur initiale)
P1 (tâ‰ˆ0.33) : ContrÃ´le pente dÃ©but
P2 (tâ‰ˆ0.67) : ContrÃ´le pente fin
P3 (t=1)    : Point d'arrivÃ©e (valeur finale)
```

**Exemple : tau_c curve pour "balanced" profile**
```json
[
  {"t": 0.0,  "value": 0.50},  // P0: Start at 0.5 (neutral temp)
  {"t": 0.33, "value": 0.45},  // P1: Dip slightly
  {"t": 0.67, "value": 0.55},  // P2: Rise slightly
  {"t": 1.0,  "value": 0.50}   // P3: Return to 0.5
]
```

**Visualisation :**
```
value
1.0 â”‚
    â”‚
0.5 â”‚  â€¢â”€â”€â”€â”€â”€â€¢â”€â”€â”€â”€â”€â€¢  (gentle wave)
    â”‚
0.0 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> t
      0    0.5    1.0
```

### CrÃ©er un profil personnalisÃ©

**Ã‰tape 1 : DÃ©finir les objectifs**
- DÃ©but de conversation : CrÃ©atif ou prÃ©cis ?
- Milieu : Maintenir ou ajuster ?
- Fin : Converger ou explorer ?

**Ã‰tape 2 : Choisir les valeurs**
```python
# Exemple : Profile "creative_to_precise"
tau_c = [
    {"t": 0.0,  "value": 0.9},   # Start high (creative)
    {"t": 0.33, "value": 0.85},  # Stay high early
    {"t": 0.67, "value": 0.5},   # Drop mid-conversation
    {"t": 1.0,  "value": 0.3}    # End low (precise)
]
```

**Ã‰tape 3 : Valider**
```python
from core.physics.bezier import CubicBezier, BezierPoint

points = [BezierPoint(**p) for p in tau_c]
curve = CubicBezier(points)

# Preview
for t in [0.0, 0.25, 0.5, 0.75, 1.0]:
    print(f"t={t:.2f} â†’ tau_c={curve.evaluate(t):.3f}")
```

**Ã‰tape 4 : InsÃ©rer en base**
```sql
INSERT INTO bezier_profiles (name, description, tau_c_json, rho_json, delta_r_json)
VALUES (
    'creative_to_precise',
    'Start creative, end precise',
    '[{"t":0.0,"value":0.9}, ...]',
    '...',  -- dÃ©finir rho
    '...'   -- dÃ©finir delta_r
);
```

### Time Mapping

Le mapping temps convertit le nombre de messages en `t âˆˆ [0, 1]`.

**Linear :**
```python
t = n / max_messages
# t=0.5 atteint Ã  mi-conversation
```

**Logarithmic (dÃ©faut) :**
```python
t = log(1 + n) / log(1 + max_messages)
# ProgrÃ¨s plus lent en dÃ©but (plus de temps pour ajustement)
```

**Sigmoid :**
```python
x = (n / max_messages - 0.5) * 10
t = 1 / (1 + exp(-x))
# Courbe en S, progression douce dÃ©but/fin
```

**Comparaison (max_messages=100) :**

| Messages | Linear | Logarithmic | Sigmoid |
|----------|--------|-------------|---------|
| 10 | 0.10 | 0.23 | 0.02 |
| 25 | 0.25 | 0.42 | 0.12 |
| 50 | 0.50 | 0.62 | 0.50 |
| 75 | 0.75 | 0.78 | 0.88 |
| 100 | 1.00 | 1.00 | 1.00 |

**Recommandation :**
- **Linear** : Conversations courtes, progression rÃ©guliÃ¨re
- **Logarithmic** : Conversations longues, adaptation prÃ©coce
- **Sigmoid** : Transitions douces, Ã©viter changements brusques

---

## SystÃ¨me de conscience

### Architecture Ã  3 phases

```
Phase 1: Passive Monitoring
    â”‚ Calcule mÃ©triques, aucune action
    â†“
Phase 2: Active Adaptation
    â”‚ HÃ©rite Phase 1 + suggÃ¨re ajustements
    â†“
Phase 3: Semantic Memory
    â”‚ HÃ©rite Phase 2 + rappel par similaritÃ©
```

### ImplÃ©mentation

**Phase 1 : ConsciousnessMonitor**

```python
# services/consciousness/metrics.py

class ConsciousnessMonitor:
    async def compute_metrics(
        self,
        context: GraphContext,
        response: str,
        physics_state: PhysicsState,
        message_index: int
    ) -> ConsciousnessMetrics:

        # 1. Coherence (semantic density)
        coherence = min(1.0, context.total_weight / 10.0)

        # 2. Tension (system stress)
        tension = physics_state.tau_c * math.log(1 + len(response) / 500)
        tension = min(1.0, tension)

        # 3. Fit (length alignment)
        expected_length = 100 + 400 * physics_state.tau_c
        actual_length = len(response)
        fit = 1.0 - abs(expected - actual) / max(expected, actual)

        # 4. Pressure (exploration vs exploitation)
        pressure = (physics_state.tau_c + physics_state.delta_r) / 2.0

        # 5. Stability (composite)
        stability = (coherence + (1 - tension) + fit) / 3.0

        return ConsciousnessMetrics(
            coherence=coherence,
            tension=tension,
            fit=fit,
            pressure=pressure,
            stability_score=stability
        )
```

**Phase 2 : AdaptiveConsciousness**

```python
# services/consciousness/adaptation.py

class AdaptiveConsciousness(ConsciousnessMonitor):
    def suggest_adaptation(
        self,
        metrics: ConsciousnessMetrics,
        state: PhysicsState,
        message_index: int
    ) -> Optional[AdaptationSuggestion]:

        # Rule 1: High tension
        if metrics.tension > 0.75:
            return AdaptationSuggestion(
                reason="High tension detected",
                adjustments={"tau_c": -0.05}  # Reduce by 5%
            )

        # Rule 2: Low coherence
        if metrics.coherence < 0.3:
            adjustment = 0.05 if state.rho < 0.5 else -0.05
            return AdaptationSuggestion(
                reason="Low coherence",
                adjustments={"rho": adjustment}
            )

        # Rule 3: High fit + stability
        if metrics.fit > 0.8 and metrics.stability_score > 0.7:
            return AdaptationSuggestion(
                reason="Stable performance, encourage exploration",
                adjustments={"tau_c": 0.03}
            )

        # Rule 4: High pressure
        if metrics.pressure > 0.85:
            return AdaptationSuggestion(
                reason="High pressure",
                adjustments={"tau_c": -0.075, "delta_r": -0.05}
            )

        # Rule 5: Long session convergence
        if message_index > 30 and 0.4 <= metrics.tension <= 0.6:
            return None  # No change, converged

        return None
```

**Phase 3 : SemanticMemory**

```python
# services/consciousness/memory.py

class SemanticMemory(AdaptiveConsciousness):
    def __init__(self):
        super().__init__()
        self._memories: Dict[str, List[MemoryEntry]] = {}

    async def record(
        self,
        session_id: str,
        content: str,
        message_index: int
    ):
        # Generate embedding
        embedding = await get_embeddings(content)

        entry = MemoryEntry(
            content=content,
            embedding=embedding,
            timestamp=datetime.utcnow(),
            message_index=message_index
        )

        # Store (limit to 50 per session)
        if session_id not in self._memories:
            self._memories[session_id] = []

        self._memories[session_id].append(entry)
        if len(self._memories[session_id]) > 50:
            self._memories[session_id].pop(0)  # Remove oldest

    async def recall(
        self,
        session_id: str,
        query_text: str,
        current_index: int,
        threshold: float = 0.7,
        max_results: int = 3
    ) -> List[MemoryEcho]:

        if session_id not in self._memories:
            return []

        # Generate query embedding
        query_emb = await get_embeddings(query_text)

        # Compute similarities
        matches = []
        for entry in self._memories[session_id]:
            # Cosine similarity
            similarity = np.dot(query_emb, entry.embedding) / (
                np.linalg.norm(query_emb) * np.linalg.norm(entry.embedding)
            )

            # Temporal decay
            turns_ago = current_index - entry.message_index
            decay = max(0.5, 1.0 - turns_ago * 0.01)
            adjusted_similarity = similarity * decay

            if adjusted_similarity >= threshold:
                matches.append(MemoryEcho(
                    content=entry.content,
                    similarity=similarity,
                    turns_ago=turns_ago
                ))

        # Return top N
        matches.sort(key=lambda x: x.similarity, reverse=True)
        return matches[:max_results]
```

### Activation

```python
# app/api/chat.py

# Phase 0: Aucune conscience
if consciousness_level == 0:
    # Standard response only
    pass

# Phase 1: Observer
elif consciousness_level == 1:
    monitor = ConsciousnessMonitor()
    metrics = await monitor.compute_metrics(context, response, state, index)
    # Return metrics in response

# Phase 2: Adaptive
elif consciousness_level == 2:
    adaptive = AdaptiveConsciousness()
    metrics = await adaptive.compute_metrics(...)
    suggestion = adaptive.suggest_adaptation(metrics, state, index)
    # Return metrics + suggestion

# Phase 3: Memory
elif consciousness_level == 3:
    memory = SemanticMemory()
    # Record user + assistant messages
    await memory.record(session_id, user_message, index)
    await memory.record(session_id, assistant_response, index + 1)
    # Recall similar messages
    echoes = await memory.recall(session_id, user_message, index)
    # Inject into context
    # Return metrics + suggestion + echoes
```

---

## Base de donnÃ©es

### SchÃ©ma relationnel

```
concepts â”€â”
          â”‚
          â”œâ”€< semantic_relations >â”€ (source, target, weight)
          â”‚
          â””â”€> embeddings (optionnel)

sessions â”€â”
          â”‚
          â”œâ”€< events >â”€ (role, content, timestamp, physics_state)
          â”‚
          â””â”€< trajectory_log >â”€ (t, tau_c, rho, delta_r, kappa)

bezier_profiles â”€> (tau_c_json, rho_json, delta_r_json, kappa_json)
```

### Indexes et performance

**Indexes critiques :**
```sql
-- Relations : O(log N) pour requÃªtes de voisins
CREATE INDEX idx_relations_source ON semantic_relations(source);
CREATE INDEX idx_relations_target ON semantic_relations(target);

-- Events : O(log N) pour historique de session
CREATE INDEX idx_events_session ON events(session_id, timestamp);
CREATE INDEX idx_events_index ON events(session_id, message_index);

-- Sessions : Cleanup queries
CREATE INDEX idx_sessions_created ON sessions(created_at);
```

**Analyse de performance :**
```sql
-- VÃ©rifier utilisation des index
EXPLAIN QUERY PLAN
SELECT target, weight
FROM semantic_relations
WHERE source = 'quantum_physics'
ORDER BY weight DESC
LIMIT 15;

-- Output attendu :
-- SEARCH TABLE semantic_relations USING INDEX idx_relations_source (source=?)
```

### Optimisations PRAGMA

```python
# database/engine.py

async def initialize(self):
    async with aiosqlite.connect(self._db_path) as db:
        # WAL mode : concurrent reads
        await db.execute("PRAGMA journal_mode=WAL")

        # Cache size : 64MB
        await db.execute("PRAGMA cache_size=-64000")

        # Memory-mapped I/O : 256MB
        await db.execute("PRAGMA mmap_size=268435456")

        # Synchronous : NORMAL (balance safety/speed)
        await db.execute("PRAGMA synchronous=NORMAL")

        # Temp store : memory
        await db.execute("PRAGMA temp_store=MEMORY")
```

### Maintenance

**Vacuum (dÃ©fragmentation) :**
```python
async def vacuum(self):
    """Defragment and optimize database."""
    async with aiosqlite.connect(self._db_path) as db:
        await db.execute("VACUUM")
        await db.execute("ANALYZE")
```

**ExÃ©cution automatique (config.yaml) :**
```yaml
database:
  vacuum_interval_days: 7
```

**Backups automatiques :**
```python
async def backup(self, backup_path: str):
    """Backup database to file."""
    async with aiosqlite.connect(self._db_path) as db:
        async with aiosqlite.connect(backup_path) as backup_db:
            await db.backup(backup_db)
```

---

## Tests et benchmarks

### Structure

```
tests/
â”œâ”€â”€ test_ab_metrics.py        # Unit tests
â””â”€â”€ benchmarks/
    â”œâ”€â”€ benchmark_phase_1.py  # Consciousness metrics
    â”œâ”€â”€ benchmark_phase_2.py  # Adaptation
    â””â”€â”€ benchmark_phase_3.py  # Memory
```

### ExÃ©cuter les tests

```bash
# Unit tests
pytest tests/test_ab_metrics.py -v

# Benchmarks Phase 1
python tests/benchmarks/benchmark_phase_1.py

# Benchmarks complets
python tests/benchmarks/benchmark_suite.py
```

### MÃ©triques de benchmark

**Latence (Phase 0-3) :**
- Phase 0 : ~1.2s baseline
- Phase 1 : +100ms (metrics)
- Phase 2 : +150ms (adaptation)
- Phase 3 : +250ms (embeddings)

**Throughput :**
- Concurrent requests : ~50 req/s (level 0)
- Database queries : ~1000 queries/s (indexed)

### Ajouter des tests

**Unit test example :**
```python
# tests/test_my_feature.py

import pytest
from app.models import ChatRequest

def test_chat_request_validation():
    # Valid request
    req = ChatRequest(message="Hello", consciousness_level=2)
    assert req.consciousness_level == 2

    # Invalid level
    with pytest.raises(ValueError):
        ChatRequest(message="Hello", consciousness_level=5)

@pytest.mark.asyncio
async def test_semantic_memory():
    from services.consciousness.memory import SemanticMemory

    memory = SemanticMemory()
    await memory.record("session1", "I love Python", 1)
    echoes = await memory.recall("session1", "What do I love?", 2)

    assert len(echoes) > 0
    assert "Python" in echoes[0].content
```

---

## Contribution

### Workflow

1. **Fork le dÃ©pÃ´t**
   ```bash
   # Via GitHub UI
   ```

2. **Cloner votre fork**
   ```bash
   git clone https://github.com/YOUR_USERNAME/lyra_clean_bis.git
   cd lyra_clean_bis
   ```

3. **CrÃ©er une branche**
   ```bash
   git checkout -b feature/my-awesome-feature
   ```

4. **DÃ©velopper**
   - Suivez les conventions de code
   - Ajoutez des tests
   - Documentez les changements

5. **Commiter**
   ```bash
   git add .
   git commit -m "Add: my awesome feature"
   ```

6. **Pusher**
   ```bash
   git push origin feature/my-awesome-feature
   ```

7. **Ouvrir une Pull Request**
   - Via GitHub UI
   - Remplissez le template de PR

### Conventions de code

**Python style :**
- PEP 8 compliant
- Type hints partout
- Docstrings (Google style)
- Max line length : 100 caractÃ¨res

**Commit messages :**
```
<type>: <description>

[optional body]

Types: Add, Fix, Update, Refactor, Docs, Test, Chore
```

**Exemples :**
```
Add: semantic memory recall with temporal decay
Fix: retry logic bug in llm_client.py
Update: increase default consciousness level to 1
Docs: add developer guide for BÃ©zier engine
```

### Pull Request template

```markdown
## Description
Brief description of changes

## Type of change
- [ ] Bug fix
- [ ] New feature
- [ ] Breaking change
- [ ] Documentation

## Testing
- [ ] Unit tests added/updated
- [ ] Manual testing performed
- [ ] Benchmarks run

## Checklist
- [ ] Code follows style guidelines
- [ ] Self-review performed
- [ ] Comments added for complex code
- [ ] Documentation updated
```

### Code review process

1. **Automated checks** (CI/CD)
   - Linting (flake8)
   - Type checking (mypy)
   - Tests (pytest)

2. **Manual review**
   - Mainteneurs review code
   - Commentaires et suggestions
   - Discussion si nÃ©cessaire

3. **Approval**
   - Au moins 1 approval requis
   - Tous les checks passed

4. **Merge**
   - Squash and merge (preferred)
   - Rebase and merge (pour features complexes)

---

## Roadmap

### Phase 4 : Persistent Memory (Q2 2025)

**Objectifs :**
- Persister la mÃ©moire sÃ©mantique en SQLite
- Limites par utilisateur (pas seulement session)
- Oubli progressif (decay exponentiel)

**Schema :**
```sql
CREATE TABLE semantic_memory (
    id INTEGER PRIMARY KEY,
    user_id TEXT,
    content TEXT,
    embedding BLOB,
    timestamp TEXT,
    access_count INTEGER,
    last_accessed TEXT
);
```

### Phase 5 : Multi-modal support (Q3 2025)

**Objectifs :**
- Support images (vision models)
- Embeddings multi-modaux (CLIP)
- Trajectoires BÃ©zier pour paramÃ¨tres visuels

### Phase 6 : Distributed deployment (Q4 2025)

**Objectifs :**
- Horizontal scaling (Redis pour mÃ©moire)
- Load balancing
- Multi-tenant support

### Contributions welcome

Consultez les [GitHub Issues](https://github.com/yourusername/lyra_clean_bis/issues) pour :
- ğŸ› Bugs Ã  corriger
- âœ¨ Features Ã  implÃ©menter
- ğŸ“š Documentation Ã  amÃ©liorer
- ğŸ¨ AmÃ©liorations UI

---

## Ressources

### Documentation externe

- [FastAPI docs](https://fastapi.tiangolo.com/)
- [Ollama API](https://github.com/ollama/ollama/blob/main/docs/api.md)
- [BÃ©zier curves (Wikipedia)](https://en.wikipedia.org/wiki/B%C3%A9zier_curve)
- [SQLite optimization](https://www.sqlite.org/optoverview.html)

### Papers

- **Consciousness metrics** : Epistemological approaches to AI introspection
- **Ballistic trajectories** : Deterministic parameter control vs reactive feedback

### Community

- ğŸ’¬ [GitHub Discussions](https://github.com/yourusername/lyra_clean_bis/discussions)
- ğŸ“§ Mailing list : lyra-dev@example.com
- ğŸ¦ Twitter : @lyra_clean

---

**Prochaine Ã©tape :** Consultez la [Configuration](CONFIGURATION.md) pour personnaliser Lyra.
