# Guide Utilisateur Lyra Clean

## Table des mati√®res

1. [Introduction](#introduction)
2. [Installation](#installation)
3. [D√©marrage rapide](#d√©marrage-rapide)
4. [Utilisation de l'API](#utilisation-de-lapi)
5. [Niveaux de conscience](#niveaux-de-conscience)
6. [Profils B√©zier](#profils-b√©zier)
7. [Gestion des sessions](#gestion-des-sessions)
8. [Interface Web](#interface-web)
9. [D√©pannage](#d√©pannage)
10. [FAQ](#faq)

---

## Introduction

Lyra Clean est un syst√®me conversationnel LLM qui contr√¥le la g√©n√©ration de texte via des trajectoires math√©matiques (courbes de B√©zier) plut√¥t que des r√©glages statiques. Cela permet un comportement dynamique et pr√©visible tout au long d'une conversation.

### Pourquoi Lyra Clean ?

**Probl√®me traditionnel :**
- Param√®tres LLM statiques (temp√©rature fixe = 0.7)
- Comportement impr√©visible dans les longues conversations
- Ajustements r√©actifs difficiles √† calibrer

**Solution Lyra :**
- Trajectoires B√©zier d√©finissant l'√©volution des param√®tres
- Comportement balistique pr√©visible (comme une trajectoire physique)
- Trois niveaux de conscience pour adaptation contextuelle

### Concepts cl√©s

- **Physique d√©terministe** : Les param√®tres √©voluent selon des courbes math√©matiques
- **Conscience √©pistemologique** : Le syst√®me s'observe et s'adapte
- **Contexte s√©mantique** : Injection intelligente de connaissances depuis un graphe
- **M√©moire s√©mantique** : Rappel d'anciennes conversations par similarit√©

---

## Installation

### Pr√©requis

1. **Python 3.10 ou sup√©rieur**
   ```bash
   python --version  # Doit afficher Python 3.10.x ou plus
   ```

2. **Ollama install√© et en cours d'ex√©cution**
   - T√©l√©chargez depuis [ollama.ai](https://ollama.ai/)
   - Installez et lancez le service
   - V√©rifiez : `ollama list`

3. **Mod√®le LLM disponible**
   ```bash
   ollama pull gpt-oss:20b
   # Ou utilisez un autre mod√®le et modifiez config.yaml
   ```

### Installation standard

```bash
# 1. Cloner le d√©p√¥t
git clone https:/SimonBouhier/github.com//lyra_clean_bis.git
cd lyra_clean_bis

# 2. Cr√©er un environnement virtuel
python -m venv venv

# 3. Activer l'environnement
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# 4. Installer les d√©pendances
pip install -r requirements.txt

# 5. Initialiser la base de donn√©es
python -c "from database.engine import ISpaceDB; import asyncio; asyncio.run(ISpaceDB('data/ispace.db').initialize())"
```

### Installation Docker (alternative)

```bash
# Construire et lancer
docker-compose up --build

# En arri√®re-plan
docker-compose up -d
```

---

## D√©marrage rapide

### Lancer le serveur

**Option 1 : Script automatique (Windows)**
```bash
start_server.bat
```

**Option 2 : Manuel**
```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

**Option 3 : Docker**
```bash
docker-compose up
```

Le serveur d√©marre sur : **http://localhost:8000**

### V√©rifier le statut

```bash
# Health check
curl http://localhost:8000/health

# R√©ponse attendue :
{
  "status": "healthy",
  "database": {"connected": true, "concepts": 1234},
  "ollama": {"connected": true, "model": "gpt-oss:20b"}
}
```

### Premi√®re conversation

```bash
curl -X POST http://localhost:8000/chat/message \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Bonjour Lyra, qui es-tu ?",
    "consciousness_level": 0
  }'
```

**R√©ponse :**
```json
{
  "response": "Je suis Lyra Clean, un syst√®me conversationnel...",
  "session_id": "abc123...",
  "message_index": 1,
  "physics_state": {
    "t": 0.01,
    "tau_c": 0.45,
    "rho": 0.5,
    "delta_r": 0.3
  },
  "metadata": {
    "latency_ms": 1234,
    "tokens": {"prompt": 45, "completion": 120}
  }
}
```

---

## Utilisation de l'API

### Endpoint principal : Chat

**POST /chat/message**

Envoie un message et re√ßoit une r√©ponse.

#### Param√®tres de requ√™te

```json
{
  "message": "Votre message ici",
  "session_id": "abc123...",           // Optionnel : r√©utiliser une session
  "consciousness_level": 2,            // 0-3 (d√©faut: 0)
  "profile_name": "balanced",          // Profil B√©zier (d√©faut: "balanced")
  "max_history": 10,                   // Messages de contexte (d√©faut: 20)
  "max_context_length": 200            // Taille max contexte s√©mantique
}
```

#### Champs obligatoires
- `message` : Le message utilisateur (string, max 10000 caract√®res)

#### Champs optionnels
- `session_id` : ID de session existante (auto-g√©n√©r√© si omis)
- `consciousness_level` : Niveau d'introspection (0-3, d√©faut 0)
- `profile_name` : Nom du profil B√©zier √† utiliser
- `max_history` : Nombre de messages pr√©c√©dents √† inclure
- `max_context_length` : Taille maximale du contexte s√©mantique inject√©

#### R√©ponse

```json
{
  "response": "R√©ponse g√©n√©r√©e",
  "session_id": "abc123...",
  "message_index": 5,
  "physics_state": {
    "t": 0.05,                // Temps normalis√© [0, 1]
    "tau_c": 0.52,            // Tension/temp√©rature
    "rho": 0.48,              // Focus/polarit√©
    "delta_r": 0.35,          // Planification
    "kappa": 0.6              // Courbure/style (optionnel)
  },
  "consciousness": {          // Si consciousness_level >= 1
    "coherence": 0.82,
    "tension": 0.45,
    "fit": 0.91,
    "pressure": 0.38,
    "stability_score": 0.87,
    "suggestion": null        // Si niveau 2, contient ajustements sugg√©r√©s
  },
  "memory_echoes": [          // Si consciousness_level >= 3
    {
      "content": "Message rappel√© du pass√©",
      "similarity": 0.89,
      "turns_ago": 12
    }
  ],
  "semantic_context": [       // Contexte inject√© depuis le graphe
    "concept_a (weight=0.82)",
    "concept_b (weight=0.75)"
  ],
  "metadata": {
    "latency_ms": 1234,
    "tokens": {
      "prompt": 456,
      "completion": 123,
      "total": 579
    }
  }
}
```

### Exemples pratiques

#### Conversation simple (niveau 0)

```python
import requests

response = requests.post("http://localhost:8000/chat/message", json={
    "message": "Explique-moi la relativit√© restreinte",
    "consciousness_level": 0
})

print(response.json()["response"])
```

#### Conversation avec m√©moire (niveau 3)

```python
# Premier message
r1 = requests.post("http://localhost:8000/chat/message", json={
    "message": "Je m'appelle Alice et j'aime la physique quantique",
    "consciousness_level": 3
})
session_id = r1.json()["session_id"]

# Plus tard dans la conversation...
r2 = requests.post("http://localhost:8000/chat/message", json={
    "message": "Quel est mon nom et mes int√©r√™ts ?",
    "session_id": session_id,
    "consciousness_level": 3
})

# Lyra devrait se souvenir gr√¢ce √† la m√©moire s√©mantique
print(r2.json()["response"])
# Devrait mentionner "Alice" et "physique quantique"

# V√©rifier les memory echoes
print(r2.json()["memory_echoes"])
```

#### Profil agressif pour brainstorming

```python
response = requests.post("http://localhost:8000/chat/message", json={
    "message": "Propose 10 id√©es originales pour un roman de SF",
    "profile_name": "aggressive",  # Haute temp√©rature, exploratoire
    "consciousness_level": 2       # Adaptatif
})
```

---

## Niveaux de conscience

Lyra poss√®de 4 niveaux de conscience (0-3) qui d√©terminent son degr√© d'introspection et d'adaptation.

### Niveau 0 : Passif

**Comportement :**
- Aucune introspection
- G√©n√©ration standard uniquement
- Performance maximale (aucun calcul suppl√©mentaire)

**Utilisation :**
- Conversations simples
- Requ√™tes factuelles
- Performance critique

**Exemple :**
```bash
curl -X POST http://localhost:8000/chat/message \
  -H "Content-Type: application/json" \
  -d '{"message": "Quelle est la capitale de la France ?", "consciousness_level": 0}'
```

### Niveau 1 : Observateur

**Comportement :**
- Calcul de m√©triques √©pistemologiques
- Aucune action, monitoring uniquement
- Retourne m√©triques dans la r√©ponse

**M√©triques calcul√©es :**
- **Coherence** (0-1) : Densit√© s√©mantique du contexte inject√©
- **Tension** (0-1) : Stress syst√®me (temp√©rature √ó longueur r√©ponse)
- **Fit** (0-1) : Alignement longueur attendue/r√©elle
- **Pressure** (0-1) : Exploration vs exploitation
- **Stability Score** (0-1) : Score composite de stabilit√©

**Utilisation :**
- Debugging de comportement
- Analyse de performance
- Recherche sur la conscience artificielle

**Exemple :**
```json
{
  "message": "Parle-moi de m√©canique quantique",
  "consciousness_level": 1
}

// R√©ponse inclut :
{
  "consciousness": {
    "coherence": 0.75,
    "tension": 0.42,
    "fit": 0.88,
    "pressure": 0.31,
    "stability_score": 0.79
  }
}
```

### Niveau 2 : Adaptatif

**Comportement :**
- H√©rite du niveau 1 (m√©triques)
- **Applique automatiquement** les ajustements aux param√®tres B√©zier (œÑ_c, œÅ, Œ¥_r)
- Boucle de feedback : m√©triques de l'interaction N-1 adaptent l'interaction N
- Modifications graduelles (5-7.5% par tour)

**R√®gles d'adaptation :**

1. **Tension √©lev√©e (> 0.75)**
   - R√©duit œÑ_c de 5% (diminue temp√©rature)
   - Raison : Stabiliser le syst√®me

2. **Coh√©rence faible (< 0.3)**
   - Ajuste œÅ vers focus
   - Raison : Am√©liorer pertinence contextuelle

3. **Fit √©lev√© (> 0.8) + Stabilit√© (> 0.7)**
   - Encourage exploration (augmente œÑ_c)
   - Raison : √âviter sur-optimisation

4. **Pression √©lev√©e (> 0.85)**
   - R√©duit œÑ_c de 7.5% et Œ¥_r de 5%
   - Raison : All√©ger charge syst√®me

5. **Session longue (> 30 messages) + tension stable**
   - Aucun changement
   - Raison : Convergence atteinte

**Utilisation :**
- Conversations longues et complexes
- Auto-ajustement en temps r√©el
- Optimisation automatique

**Exemple :**
```json
{
  "message": "Continue la discussion",
  "consciousness_level": 2
}

// R√©ponse peut inclure :
{
  "consciousness": {
    "suggestion": {
      "reason": "High tension detected",
      "adjustments": {
        "tau_c": -0.05  // R√©duit de 5%
      }
    }
  }
}
```

### Niveau 3 : M√©moire s√©mantique

**Comportement :**
- H√©rite du niveau 2 (m√©triques + adaptation)
- Enregistre chaque message avec embeddings (1024D)
- Rappelle messages similaires par cosine similarity
- Applique d√©croissance temporelle : `max(0.5, 1.0 - turns_ago * 0.01)`

**Fonctionnement :**

1. **Enregistrement :**
   - Chaque message ‚Üí embeddings mxbai-embed-large
   - Stockage en m√©moire (dict : session_id ‚Üí entries)
   - Limite : 50 entr√©es par session

2. **Rappel :**
   - Calcul similarit√© cosinus avec message actuel
   - Seuil : 0.7 minimum
   - Limite : 3 meilleurs matches
   - D√©croissance : -1% par tour √©coul√©

3. **Injection :**
   - Ajout√© au contexte syst√®me comme `[MEMORY ECHO]`
   - Format : contenu + metadata (similarit√©, anciennet√©)

**Utilisation :**
- Conversations multi-tours avec continuit√©
- Questions de suivi sur sujets pass√©s
- Contexte personnel maintenu

**Exemple :**
```python
# Message initial
r1 = requests.post("http://localhost:8000/chat/message", json={
    "message": "Mon chien s'appelle Rex et il adore jouer au frisbee",
    "consciousness_level": 3
})
session_id = r1.json()["session_id"]

# 20 messages plus tard...
r2 = requests.post("http://localhost:8000/chat/message", json={
    "message": "Comment s'appelle mon chien d√©j√† ?",
    "session_id": session_id,
    "consciousness_level": 3
})

# V√©rifie memory echoes
echoes = r2.json()["memory_echoes"]
# [{"content": "Mon chien s'appelle Rex...", "similarity": 0.91, "turns_ago": 20}]
```

**Limitations :**
- ‚ö†Ô∏è M√©moire en RAM uniquement (perdue au red√©marrage serveur)
- ‚ö†Ô∏è Limite 50 messages par session
- ‚ö†Ô∏è Co√ªt : +100ms par requ√™te (g√©n√©ration embeddings)

### Comparaison des niveaux

| Niveau | M√©triques | Adaptation | M√©moire | Latence | Usage |
|--------|-----------|------------|---------|---------|-------|
| 0 | ‚ùå | ‚ùå | ‚ùå | ~1.2s | Production rapide |
| 1 | ‚úÖ | ‚ùå | ‚ùå | ~1.3s | Monitoring/debug |
| 2 | ‚úÖ | ‚úÖ | ‚ùå | ~1.4s | Conversations complexes |
| 3 | ‚úÖ | ‚úÖ | ‚úÖ | ~1.5s | M√©moire long terme |

---

## Profils B√©zier

Les profils d√©finissent comment les param√®tres √©voluent au cours d'une conversation via des courbes de B√©zier cubiques.

### Profils disponibles

#### 1. Balanced (d√©faut)

**Caract√©ristiques :**
- Temp√©rature stable autour de 0.7
- Pas de d√©rive extr√™me
- Bon pour usage g√©n√©ral

**Courbes :**
```yaml
tau_c:  [0, 0.5] ‚Üí [0.5, 0.45] ‚Üí [0.55, 0.55] ‚Üí [1, 0.5]
rho:    [0, 0.5] ‚Üí [0.33, 0.5] ‚Üí [0.67, 0.5] ‚Üí [1, 0.5]
delta_r:[0, 0.3] ‚Üí [0.33, 0.35] ‚Üí [0.67, 0.35] ‚Üí [1, 0.3]
```

**Usage :**
```json
{"message": "...", "profile_name": "balanced"}
```

#### 2. Aggressive

**Caract√©ristiques :**
- Haute temp√©rature initiale (0.8-1.0)
- Exploratoire, cr√©atif
- Bon pour brainstorming

**Usage :**
- G√©n√©ration d'id√©es
- √âcriture cr√©ative
- Exploration de concepts

**Exemple :**
```json
{
  "message": "Invente 5 cr√©atures fantastiques originales",
  "profile_name": "aggressive"
}
```

#### 3. Conservative

**Caract√©ristiques :**
- Basse temp√©rature (0.3-0.5)
- Pr√©cis, factuel
- Bon pour t√¢ches analytiques

**Usage :**
- Code generation
- R√©sum√©s factuels
- Calculs math√©matiques

**Exemple :**
```json
{
  "message": "√âcris une fonction Python pour trier une liste",
  "profile_name": "conservative"
}
```

### Cr√©er un profil personnalis√©

Les profils sont d√©finis dans la base de donn√©es. Vous pouvez en cr√©er via SQL :

```sql
INSERT INTO bezier_profiles (name, description, tau_c_json, rho_json, delta_r_json)
VALUES (
  'custom_profile',
  'Mon profil personnalis√©',
  '[{"t": 0.0, "value": 0.6}, {"t": 0.33, "value": 0.7}, {"t": 0.67, "value": 0.5}, {"t": 1.0, "value": 0.4}]',
  '[{"t": 0.0, "value": 0.5}, {"t": 0.33, "value": 0.5}, {"t": 0.67, "value": 0.5}, {"t": 1.0, "value": 0.5}]',
  '[{"t": 0.0, "value": 0.3}, {"t": 0.33, "value": 0.4}, {"t": 0.67, "value": 0.3}, {"t": 1.0, "value": 0.2}]'
);
```

**Contraintes :**
- Exactement 4 points de contr√¥le (t=0, t‚âà0.33, t‚âà0.67, t=1)
- `t` doit √™tre strictement croissant
- `value` ‚àà [0, 1]

### Lister les profils

```bash
curl http://localhost:8000/profiles

# R√©ponse :
{
  "profiles": [
    {
      "name": "balanced",
      "description": "Balanced temperature and focus",
      "parameters": ["tau_c", "rho", "delta_r"]
    },
    ...
  ]
}
```

### Pr√©visualiser un profil

```bash
curl "http://localhost:8000/profiles/balanced?preview=20"

# Retourne 20 points √©chantillonn√©s de la trajectoire
{
  "name": "balanced",
  "trajectory": [
    {"t": 0.0, "tau_c": 0.50, "rho": 0.50, "delta_r": 0.30},
    {"t": 0.05, "tau_c": 0.48, "rho": 0.50, "delta_r": 0.31},
    ...
  ]
}
```

---

## Gestion des sessions

### Cr√©er une session

**POST /sessions**

```json
{
  "profile_name": "balanced",
  "max_messages": 100,
  "time_mapping": "logarithmic"
}
```

**R√©ponse :**
```json
{
  "session_id": "abc123...",
  "created_at": "2025-01-14T10:30:00Z",
  "profile_name": "balanced"
}
```

### R√©cup√©rer une session

**GET /sessions/{session_id}**

```bash
curl http://localhost:8000/sessions/abc123...
```

**R√©ponse :**
```json
{
  "session_id": "abc123...",
  "created_at": "2025-01-14T10:30:00Z",
  "message_count": 15,
  "profile_name": "balanced",
  "time_mapping": "logarithmic"
}
```

### Historique de conversation

**GET /sessions/{session_id}/history**

```bash
curl http://localhost:8000/sessions/abc123.../history
```

**R√©ponse :**
```json
{
  "session_id": "abc123...",
  "messages": [
    {
      "role": "user",
      "content": "Bonjour",
      "timestamp": "2025-01-14T10:30:15Z",
      "message_index": 1
    },
    {
      "role": "assistant",
      "content": "Bonjour ! Comment puis-je vous aider ?",
      "timestamp": "2025-01-14T10:30:17Z",
      "message_index": 2,
      "physics_state": {"t": 0.01, "tau_c": 0.50, ...}
    },
    ...
  ],
  "total_messages": 15
}
```

### Supprimer une session

**DELETE /sessions/{session_id}**

```bash
curl -X DELETE http://localhost:8000/sessions/abc123...
```

**R√©ponse :**
```json
{
  "success": true,
  "message": "Session deleted"
}
```

---

## Interface Web

Lyra inclut une interface web minimaliste pour tests rapides.

### Acc√®s

Ouvrez votre navigateur : **http://localhost:8000**

### Fonctionnalit√©s

- ‚úÖ Envoi de messages en temps r√©el
- ‚úÖ S√©lection du niveau de conscience (0-3)
- ‚úÖ S√©lection du profil B√©zier
- ‚úÖ Affichage de l'historique de conversation
- ‚úÖ M√©triques de conscience (si niveau ‚â• 1)
- ‚úÖ Memory echoes (si niveau 3)
- ‚úÖ Visualisation de l'√©tat physique

### Fichier source

L'interface est un fichier HTML statique : `app/static/index.html`

Vous pouvez la personnaliser selon vos besoins.

---

## D√©pannage

### Erreur : "Ollama server not reachable"

**Sympt√¥mes :**
```json
{
  "error": "Ollama request failed after 3 attempts"
}
```

**Solutions :**
1. V√©rifiez qu'Ollama est lanc√© :
   ```bash
   ollama list
   ```

2. V√©rifiez l'URL dans `config.yaml` :
   ```yaml
   llm:
     base_url: "http://localhost:11434"  # Port par d√©faut
   ```

3. Testez manuellement :
   ```bash
   curl http://localhost:11434/api/tags
   ```

### Erreur : "Model not found"

**Sympt√¥mes :**
```json
{
  "error": "HTTP 404: model 'gpt-oss:20b' not found"
}
```

**Solutions :**
1. T√©l√©chargez le mod√®le :
   ```bash
   ollama pull gpt-oss:20b
   ```

2. Ou modifiez `config.yaml` pour utiliser un mod√®le disponible :
   ```yaml
   llm:
     model: "llama3:latest"  # Ou autre mod√®le install√©
   ```

### Erreur : "Database locked"

**Sympt√¥mes :**
```
sqlite3.OperationalError: database is locked
```

**Solutions :**
1. V√©rifiez qu'aucun autre processus n'utilise la DB :
   ```bash
   lsof data/ispace.db  # Linux/Mac
   ```

2. Activez le mode WAL (d√©j√† fait par d√©faut) :
   ```sql
   PRAGMA journal_mode=WAL;
   ```

3. Red√©marrez le serveur.

### Performance lente

**Sympt√¥mes :**
- Latence > 5 secondes par requ√™te

**Solutions :**

1. **D√©sactivez la conscience si inutile :**
   ```json
   {"consciousness_level": 0}  // Plus rapide
   ```

2. **R√©duisez max_history :**
   ```json
   {"max_history": 5}  // Au lieu de 20
   ```

3. **V√©rifiez les ressources Ollama :**
   - CPU : Ollama utilise 100% d'un core par d√©faut
   - RAM : Mod√®le 20B n√©cessite ~12GB
   - GPU : Utilisez CUDA si disponible

4. **Optimisez la base de donn√©es :**
   ```bash
   curl -X POST http://localhost:8000/admin/vacuum
   ```

### M√©moire s√©mantique ne fonctionne pas

**Sympt√¥mes :**
- `memory_echoes: []` m√™me avec niveau 3

**Causes possibles :**

1. **Similarit√© trop faible (< 0.7)**
   - Messages trop diff√©rents
   - Solution : Reformulez pour √™tre plus explicite

2. **Trop r√©cent (d√©croissance temporelle)**
   - Attendez quelques tours
   - Solution : Testez avec >5 messages d'√©cart

3. **Session vide**
   - Premi√®re utilisation du session_id
   - Solution : Accumulez d'abord des messages

4. **Serveur red√©marr√©**
   - M√©moire en RAM perdue
   - Solution : √Ä venir (persistance SQLite)

---

## FAQ

### Q : Quelle est la diff√©rence entre œÑ_c, œÅ, Œ¥_r et Œ∫ ?

**R√©ponse :**

- **œÑ_c (tau_c)** : Tension/temp√©rature
  - Contr√¥le la cr√©ativit√© (haute) vs d√©terminisme (basse)
  - Mapp√© vers temperature Ollama : [0.1, 1.5]

- **œÅ (rho)** : Focus/polarit√©
  - Contr√¥le r√©p√©tition vs diversit√©
  - Mapp√© vers presence_penalty et frequency_penalty

- **Œ¥_r (delta_r)** : Planification/scheduling
  - Contr√¥le la densit√© de contexte inject√©e
  - Influence le nombre de voisins s√©mantiques

- **Œ∫ (kappa)** : Courbure/style (optionnel)
  - G√©n√®re des hints de style dans le prompt
  - Ex : "Be concise" ou "Elaborate deeply"

### Q : Puis-je utiliser un autre LLM qu'Ollama ?

**R√©ponse :**
Actuellement, seul Ollama est support√©. Pour ajouter un autre backend :

1. Cr√©ez un nouveau client dans `app/` (ex : `openai_client.py`)
2. Impl√©mentez la m√™me interface que `OllamaClient`
3. Modifiez `app/main.py` pour injecter le nouveau client

Contribution welcome ! üöÄ

### Q : Comment exporter une conversation ?

**R√©ponse :**
Utilisez l'endpoint d'historique :

```bash
curl http://localhost:8000/sessions/{session_id}/history > conversation.json
```

Ou directement depuis SQLite :

```bash
sqlite3 data/ispace.db "SELECT * FROM events WHERE session_id='...' ORDER BY timestamp"
```

### Q : La m√©moire s√©mantique est-elle persistante ?

**R√©ponse :**
‚ö†Ô∏è Non, actuellement elle est en RAM uniquement. Red√©marrer le serveur efface la m√©moire.

**Workaround :**
- Utilisez `consciousness_level: 0-2` pour conversations ne n√©cessitant pas de m√©moire
- Keep-alive le serveur en production

**Roadmap :**
- Phase 4 ajoutera la persistance SQLite pour la m√©moire

### Q : Combien de tokens maximum par requ√™te ?

**R√©ponse :**
Configur√© √† **4096 tokens** par d√©faut dans `app/llm_client.py:180`.

Pour modifier :
```python
# app/llm_client.py
"options": {
    "num_predict": 8192  # Augmentez si votre mod√®le le supporte
}
```

‚ö†Ô∏è V√©rifiez les limites de votre mod√®le Ollama.

### Q : Puis-je utiliser Lyra sans graphe de connaissances ?

**R√©ponse :**
Oui ! D√©sactivez l'injection de contexte dans `config.yaml` :

```yaml
context:
  enabled: false
```

Lyra fonctionnera comme un LLM standard avec gestion de sessions.

### Q : Comment ajouter des concepts au graphe s√©mantique ?

**R√©ponse :**

**Option 1 : SQL direct**
```sql
INSERT INTO concepts (concept, embedding)
VALUES ('nouveau_concept', NULL);  -- Embedding optionnel

INSERT INTO semantic_relations (source, target, weight)
VALUES ('concept_a', 'nouveau_concept', 0.8);
```

**Option 2 : Script Python**
```python
from database.engine import ISpaceDB
import asyncio

async def add_concept():
    db = ISpaceDB('data/ispace.db')
    # Utilisez les m√©thodes du db engine
    # (√† impl√©menter selon vos besoins)
```

**Option 3 : Import depuis fichier**
Consultez `scripts/build_global_map.py` pour un exemple d'import batch.

### Q : Les courbes de B√©zier sont-elles modifiables en temps r√©el ?

**R√©ponse :**
Non, un profil B√©zier est fix√© pour toute la dur√©e d'une session.

**Workaround :**
- Cr√©ez une nouvelle session avec un autre profil
- Ou impl√©mentez la modification via SQL :
  ```sql
  UPDATE sessions SET profile_name='aggressive' WHERE session_id='...';
  ```
  ‚ö†Ô∏è Cela peut cr√©er des discontinuit√©s dans les trajectoires

### Q : Quelle est la latence typique ?

**R√©ponse :**

| Configuration | Latence moyenne |
|---------------|-----------------|
| Niveau 0, pas de contexte | 1.0-1.5s |
| Niveau 1, avec contexte | 1.3-1.8s |
| Niveau 2, adaptatif | 1.4-2.0s |
| Niveau 3, avec m√©moire | 1.5-2.2s |

**Facteurs :**
- Taille du mod√®le LLM
- CPU vs GPU (Ollama)
- Longueur de l'historique
- Complexit√© du graphe s√©mantique

### Q : Puis-je h√©berger Lyra en production ?

**R√©ponse :**
Oui, mais consid√©rez ces points :

**√Ä faire avant production :**
1. ‚úÖ Restreindre CORS dans `config.yaml`
   ```yaml
   cors:
     origins:
       - "https://yourdomain.com"
   ```

2. ‚úÖ Activer l'authentification API
   ```yaml
   security:
     api_key_enabled: true
   ```

3. ‚úÖ Configurer le rate limiting
   ```yaml
   security:
     rate_limit_per_minute: 60
   ```

4. ‚úÖ Utiliser un reverse proxy (nginx, Caddy)
5. ‚úÖ Configurer HTTPS
6. ‚úÖ Monitoring (logs, m√©triques)
7. ‚úÖ Backups automatiques de la DB

**D√©ploiement Docker recommand√© :**
```bash
docker-compose -f docker-compose.prod.yml up -d
```

### Q : Comment contribuer au projet ?

**R√©ponse :**
Consultez le [Developer Guide](DEVELOPER_GUIDE.md#contribution) !

R√©sum√© :
1. Fork le d√©p√¥t
2. Cr√©ez une branche : `git checkout -b feature/ma-feature`
3. Commitez : `git commit -m "Add: ma nouvelle feature"`
4. Push : `git push origin feature/ma-feature`
5. Ouvrez une Pull Request

---

## Support

- üìñ Documentation compl√®te : [docs/fr/](.)
- üêõ Rapporter un bug : [GitHub Issues](https://github.com/yourusername/lyra_clean_bis/issues)
- üí¨ Discussions : [GitHub Discussions](https://github.com/yourusername/lyra_clean_bis/discussions)
- üìß Email : support@example.com

---

**Prochaine √©tape :** Consultez le [Developer Guide](DEVELOPER_GUIDE.md) pour contribuer ou personnaliser Lyra.

