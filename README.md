# Lyra ACE

[![License: CC BY-NC-ND 4.0](https://licensebuttons.net/l/by-nc-nd/4.0/88x31.png)](https://creativecommons.org/licenses/by-nc-nd/4.0/)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)

[ğŸ‡«ğŸ‡· FranÃ§ais](#-franÃ§ais) | [ğŸ‡¬ğŸ‡§ English](#-english)

---

## ğŸ‡«ğŸ‡· FranÃ§ais

### Vue d'ensemble

**Lyra ACE** est un systÃ¨me conversationnel LLM innovant qui utilise des **trajectoires de BÃ©zier** pour contrÃ´ler de maniÃ¨re dÃ©terministe les paramÃ¨tres de gÃ©nÃ©ration (tempÃ©rature, pÃ©nalitÃ©s) et qui a pour objectif de "chercher le 0-cochain" des systÃ¨mes cognitifs augmentÃ©s Ã  travers la construction d'un RAG partagÃ© et pondÃ©rÃ© par plusieurs moteurs.

**Philosophie de conception :**
- ğŸ¯ **Trajectoires balistiques** : Comportement prÃ©visible et ajustable
- ğŸ§  **Trois niveaux de conscience** : Passif â†’ Adaptatif â†’ MÃ©moire sÃ©mantique
- ğŸ“Š **Graphe de connaissances sÃ©mantiques** : Injection de contexte intelligent via SQLite
- âš¡ **DÃ©pendances minimalistes** : ~150MB (rÃ©duction de 77% vs versions prÃ©cÃ©dentes)
- ğŸ”¬ **Physique dÃ©terministe** : ContrÃ´le mathÃ©matique prÃ©cis des paramÃ¨tres LLM

### CaractÃ©ristiques principales

- âœ… **API FastAPI asynchrone** avec gestion de sessions persistantes
- âœ… **Moteur physique BÃ©zier** : 4 paramÃ¨tres (Ï„_c, Ï, Î´_r, Îº) contrÃ´lÃ©s par courbes cubiques
- âœ… **Injection de contexte sÃ©mantique** : Extraction TF-IDF + requÃªtes de voisinage PPMI
- âœ… **Conscience adaptative** : Ajustements automatiques basÃ©s sur mÃ©triques Ã©pistemologiques
- âœ… **MÃ©moire sÃ©mantique** : Rappel par similaritÃ© cosinus avec dÃ©croissance temporelle
- âœ… **Base de donnÃ©es SQLite optimisÃ©e** : Mode WAL, index O(log N), pooling de connexions
- âœ… **Client Ollama async** : Pooling HTTP, retry avec backoff exponentiel
- âœ… **Interface web** : UI minimaliste pour tests rapides

### Lyra-ACE (Advanced Consciousness Engine) - Phase 1

**Nouvelles fonctionnalitÃ©s :**

- âœ… **Graphe dynamique avec deltas** : Mutations incrÃ©mentielles auditables du graphe sÃ©mantique
- âœ… **Calcul Îº hybride** : Courbure Ollivier + Jaccard pour analyse structurelle
- âœ… **Rollback transactionnel** : Annulation des mutations avec historique complet
- âœ… **Wrapper multi-modÃ¨les** : GÃ©nÃ©ration sÃ©quentielle avec plusieurs LLM Ollama
- âœ… **MÃ©triques de consensus** : Comparaison et sÃ©lection automatique des meilleures rÃ©ponses
- âœ… **Limite de mutation 5%** : Protection contre les modifications massives du graphe

**Nouveaux endpoints API :**

- `POST /graph/delta` - Appliquer une mutation au graphe
- `GET /graph/kappa/{source}/{target}` - Calculer la courbure Îº hybride
- `POST /graph/rollback` - Annuler des mutations
- `GET /graph/stats` - Statistiques des mutations
- `GET /multimodel/models` - Lister les modÃ¨les Ollama disponibles
- `POST /multimodel/generate` - GÃ©nÃ©ration multi-modÃ¨les avec consensus

### DÃ©marrage rapide

#### PrÃ©requis
- Python 3.10+
- [Ollama](https://ollama.ai/) installÃ© et en cours d'exÃ©cution
- Un modÃ¨le LLM (gemma3, llama3.1, mistral, etc.)

#### Installation

```bash


#### Lancement

```bash
# Scripts par modÃ¨le (Windows) - choisir un:
start_gemma3.bat      # Gemma 3 (3.3GB) - LÃ©ger, rapide
start_mistral.bat     # Mistral (4.4GB) - Multilingue
start_llama3.bat      # Llama 3.1 8B (4.9GB) - Polyvalent
start_granite.bat     # Granite 3.3 (4.9GB) - Code, enterprise
start_deepseek.bat    # DeepSeek R1 (5.2GB) - Raisonnement
start_gptoss.bat      # GPT-OSS 20B (13GB) - Plus puissant

# Grand contexte (32k tokens)
start_large_context.bat

# Manuel avec variables d'environnement
set LYRA_MODEL=mistral:latest
set LYRA_NUM_CTX=16384
uvicorn app.main:app --host 127.0.0.1 --port 8000 --reload

# Docker
docker-compose up
```

#### Variables d'environnement

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `LYRA_MODEL` | gpt-oss:20b | ModÃ¨le Ollama Ã  utiliser |
| `LYRA_OLLAMA_URL` | http://localhost:11434 | URL du serveur Ollama |
| `LYRA_NUM_CTX` | 8192 | Taille du contexte (tokens) |

#### Premier test

```bash
# Health check
curl http://localhost:8000/health

# Envoyer un message
curl -X POST http://localhost:8000/chat/message \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Bonjour Lyra, explique-moi comment tu fonctionnes",
    "consciousness_level": 2
  }'
```

Ou ouvrez votre navigateur : http://localhost:8000

### Architecture

```
lyra_clean_bis/
â”œâ”€â”€ app/                    # Application FastAPI
â”‚   â”œâ”€â”€ main.py             # Point d'entrÃ©e
â”‚   â”œâ”€â”€ models.py           # ModÃ¨les Pydantic
â”‚   â”œâ”€â”€ llm_client.py       # Client Ollama + MultiModelClient
â”‚   â”œâ”€â”€ embeddings.py       # Wrapper embeddings
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ chat.py         # Endpoint conversation
â”‚       â”œâ”€â”€ sessions.py     # Gestion sessions
â”‚       â”œâ”€â”€ graph.py        # [ACE] Mutations graphe
â”‚       â””â”€â”€ multimodel.py   # [ACE] Multi-modÃ¨les
â”‚
â”œâ”€â”€ services/               # Couche mÃ©tier
â”‚   â”œâ”€â”€ injector.py         # Injection contexte
â”‚   â””â”€â”€ consciousness/
â”‚       â”œâ”€â”€ metrics.py      # Phase 1: MÃ©triques passives
â”‚       â”œâ”€â”€ adaptation.py   # Phase 2: Adaptation active
â”‚       â””â”€â”€ memory.py       # Phase 3: MÃ©moire sÃ©mantique
â”‚
â”œâ”€â”€ database/               # Moteur SQLite
â”‚   â”œâ”€â”€ engine.py           # ISpaceDB + mÃ©thodes delta
â”‚   â”œâ”€â”€ graph_delta.py      # [ACE] GraphDelta, KappaCalculator
â”‚   â””â”€â”€ schema.sql          # SchÃ©ma SQL (11 tables)
â”‚
â”œâ”€â”€ core/physics/           # Moteur BÃ©zier
â”‚   â””â”€â”€ bezier.py           # Trajectoires physiques
â”‚
â”œâ”€â”€ config.yaml             # Configuration centralisÃ©e
â””â”€â”€ docs/                   # Documentation complÃ¨te
    â”œâ”€â”€ fr/                 # Documentation franÃ§aise
    â””â”€â”€ en/                 # English documentation
```

### Niveaux de conscience

| Niveau | Nom | Description | CapacitÃ©s |
|--------|-----|-------------|-----------|
| 0 | **Passif** | Aucune introspection | RÃ©ponses standard uniquement |
| 1 | **Observateur** | Monitoring sans action | Calcul mÃ©triques (cohÃ©rence, tension, fit) |
| 2 | **Adaptatif** | Auto-ajustement actif | Modifie Ï„_c, Ï, Î´_r selon les mÃ©triques N-1 |
| 3 | **MÃ©moire** | Rappel sÃ©mantique | Injection [MEMORY ECHO] + adaptation niveau 2 |

### Documentation complÃ¨te

- ğŸ“˜ **[Guide utilisateur (FR)](docs/fr/USER_GUIDE.md)** : Utilisation de l'API
- ğŸ”§ **[Guide dÃ©veloppeur (FR)](docs/fr/DEVELOPER_GUIDE.md)** : Architecture et contribution
- ğŸ¨ **[RÃ©fÃ©rence API (FR)](docs/fr/API_REFERENCE.md)** : Endpoints dÃ©taillÃ©s
- âš™ï¸ **[Configuration (FR)](docs/fr/CONFIGURATION.md)** : ParamÃ¨tres systÃ¨me

### Contribuer

Les contributions sont bienvenues ! Consultez le [guide de contribution](docs/fr/DEVELOPER_GUIDE.md#contribution).

### Licence

MIT License - voir [LICENSE](LICENSE)

---

## ğŸ‡¬ğŸ‡§ English

### Overview

**Lyra ACE** is an innovative LLM conversational system that uses **BÃ©zier trajectories** to deterministically control generation parameters (temperature, penalties) rather than reactive feedback loops.

**Design philosophy:**
- ğŸ¯ **Ballistic trajectories**: Predictable and tunable behavior
- ğŸ§  **Three consciousness levels**: Passive â†’ Adaptive â†’ Semantic memory
- ğŸ“Š **Semantic knowledge graph**: Intelligent context injection via SQLite
- âš¡ **Minimalist dependencies**: ~150MB (77% reduction vs previous versions)
- ğŸ”¬ **Deterministic physics**: Precise mathematical control of LLM parameters

### Key features

- âœ… **Async FastAPI** with persistent session management
- âœ… **BÃ©zier physics engine**: 4 parameters (Ï„_c, Ï, Î´_r, Îº) controlled by cubic curves
- âœ… **Semantic context injection**: TF-IDF extraction + PPMI neighborhood queries
- âœ… **Adaptive consciousness**: Automatic adjustments based on epistemological metrics
- âœ… **Semantic memory**: Cosine similarity recall with temporal decay
- âœ… **Optimized SQLite database**: WAL mode, O(log N) indexes, connection pooling
- âœ… **Async Ollama client**: HTTP pooling, exponential backoff retry
- âœ… **Web interface**: Minimalist UI for quick testing

### Lyra-ACE (Advanced Consciousness Engine) - Phase 1

**New features:**

- âœ… **Dynamic graph with deltas**: Auditable incremental mutations of the semantic graph
- âœ… **Hybrid Îº calculation**: Ollivier + Jaccard curvature for structural analysis
- âœ… **Transactional rollback**: Undo mutations with complete history
- âœ… **Multi-model wrapper**: Sequential generation with multiple Ollama LLMs
- âœ… **Consensus metrics**: Automatic comparison and selection of best responses
- âœ… **5% mutation limit**: Protection against massive graph modifications

**New API endpoints:**

- `POST /graph/delta` - Apply a mutation to the graph
- `GET /graph/kappa/{source}/{target}` - Calculate hybrid Îº curvature
- `POST /graph/rollback` - Undo mutations
- `GET /graph/stats` - Mutation statistics
- `GET /multimodel/models` - List available Ollama models
- `POST /multimodel/generate` - Multi-model generation with consensus

### Quick start

#### Prerequisites
- Python 3.10+
- [Ollama](https://ollama.ai/) installed and running
- An LLM model (gemma3, llama3.1, mistral, etc.)

#### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/lyra_clean_bis.git
cd lyra_clean_bis

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Linux: source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

#### Launch

```bash
# Model-specific scripts (Windows) - choose one:
start_gemma3.bat      # Gemma 3 (3.3GB) - Light, fast
start_mistral.bat     # Mistral (4.4GB) - Multilingual
start_llama3.bat      # Llama 3.1 8B (4.9GB) - Versatile
start_granite.bat     # Granite 3.3 (4.9GB) - Code, enterprise
start_deepseek.bat    # DeepSeek R1 (5.2GB) - Reasoning
start_gptoss.bat      # GPT-OSS 20B (13GB) - Most powerful

# Large context (32k tokens)
start_large_context.bat

# Manual with environment variables
set LYRA_MODEL=mistral:latest
set LYRA_NUM_CTX=16384
uvicorn app.main:app --host 127.0.0.1 --port 8000 --reload

# Docker
docker-compose up
```

#### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `LYRA_MODEL` | gpt-oss:20b | Ollama model to use |
| `LYRA_OLLAMA_URL` | http://localhost:11434 | Ollama server URL |
| `LYRA_NUM_CTX` | 8192 | Context window size (tokens) |

#### First test

```bash
# Health check
curl http://localhost:8000/health

# Send message
curl -X POST http://localhost:8000/chat/message \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Hello Lyra, explain how you work",
    "consciousness_level": 2
  }'
```

Or open your browser: http://localhost:8000

### Architecture

See French section above for directory structure.

### Consciousness levels

| Level | Name | Description | Capabilities |
|-------|------|-------------|--------------|
| 0 | **Passive** | No introspection | Standard responses only |
| 1 | **Observer** | Monitoring without action | Compute metrics (coherence, tension, fit) |
| 2 | **Adaptive** | Active self-adjustment | Modifies Ï„_c, Ï, Î´_r based on N-1 metrics |
| 3 | **Memory** | Semantic recall | Inject [MEMORY ECHO] + level 2 adaptation |

### Complete documentation

- ğŸ“˜ **[User Guide (EN)](docs/en/USER_GUIDE.md)**: API usage
- ğŸ”§ **[Developer Guide (EN)](docs/en/DEVELOPER_GUIDE.md)**: Architecture and contribution
- ğŸ¨ **[API Reference (EN)](docs/en/API_REFERENCE.md)**: Detailed endpoints
- âš™ï¸ **[Configuration (EN)](docs/en/CONFIGURATION.md)**: System parameters

### Contributing

Contributions welcome! See [contribution guide](docs/en/DEVELOPER_GUIDE.md#contributing).

### License

CC-BY NC 4.0 License - see [LICENSE](LICENSE)

---

## Credits

**Author**: Simon ([GitHub Profile](https://github.com/SimonBouhier))

**Acknowledgments**:
- Ollama team for the inference engine
- FastAPI contributors
- Open-source community

## Support

- ğŸ“– Documentation: [docs/](docs/)
- ğŸ› Issues: [GitHub Issues](https://github.com/SimonBouhier/Lyra_ACE/issues)
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/SimonBouhier/Lyra_ACE/discussions)
